---
title: "[논문 리뷰] YOLOv4"
date: 2020-12-24 00:25:00 -0400
categories: yolo yolov4 object-detection spp-net panet Mish-Activation
---

# YOLOv4
- - -
YOLO는 You Only Look Once의 약자로 이미지를 한 번 보는 것만으로도 Object의 종류와 위치를 추측하는 딥러닝 기반의 물체 인식 알고리즘입니다.
이미지를 한 번만 본다는게 어떤 뜻일까요?

#### - R-CNN


![rcnn_network](https://user-images.githubusercontent.com/39725476/103012866-8b96a180-457f-11eb-8522-a2541d10df1f.png)  

대표적인 Object Detection 방법인 R-CNN의 학습 방법은 다음과 같습니다.
1. 이미지에 대한 후보 영역(region proposal) 약 2000개를 생성합니다.
2. 이 후보 영역을 wrap/crop 방법을 통해 고정된 크기로 맞춘 후 CNN을 통과시킵니다.

#### - YOLO


![yolo_system](https://user-images.githubusercontent.com/39725476/103013339-52126600-4580-11eb-8d35-f99497b3b5f4.png)  

하지만 YOLO는 region proposal과 같이 이미지를 나누지 않고 CNN에 단 한 번 통과시킵니다.
이를 두고 이미지를 한 번만 본다고 표현합니다. 또 한 번 보기 때문에 속도가 빠릅니다.

실제로 MS-COCO 데이터 기준 FPS를 보면 그 차이가 더 확실하게 보여집니다.

|  | Fast R-CNN | Faster R-CNN | YOLOv1 | YOLOv5 |
| :-----: | :-----: | :-----: | :-----: | :-----: |
| FPS | 0.5 | 5 | 45 | 140 |

그럼 본론으로 들어가서 Object Detection 중 최고의 속도를 보이는 YOLOv4 논문 리뷰를 시작하겠습니다.
- - -
## 1. Introduction

Introduction에서는 다른 신경망과 YOLOv4가 다른 점을 소개하고 있다. 논문에서 제시한 요약 3가지는 다음과 같습니다.

1. We develope an efficient and powerful object detection model. It makes everyone can use a 1080 Ti or 2080 Ti GPU to train a super fast and accurate object detector.  
효율적이고 강력한 object detection model을 개발했다. 이 모델은 단 한 개의 1080 Ti나 2080 Ti로도 매우 빠르고 정확한 object detector 학습이 가능하게 한다.  
2. We verify the influence of state-of-the-art Bag-of-Freebies and Bag-of-Specials methods of object detection during the detector training.  
모델 훈련동안에 object detection의 최첨단 BoF와 BoS 방법의 영향을 증명한다.  
3. We modify state-of-the-art methods and make them more efficient and suitable for single GPU training, including CBN. PAN, SAM, etc.  
최신기술을 수정하여 single GPU에서 CBN, PAN, SAM을 포함하여 학습하기에 더 효율적이고 적합하게 만든다.  
- - -
## 2. Related work
#### 2.1 Object detection models
Object Detection 분야의 연구는 R-CNN을 시작으로 많은 연구가 진행되고 있습니다. 그리고 대표적인 구조로 논문에서는 아래의 구조를 제시하고 있습니다.

![object-detector-architecture](https://user-images.githubusercontent.com/39725476/103016548-8ccacd00-4585-11eb-8f9d-809a1574c5e4.png)  

구조는 크게 **Backbone/Neck/Head** 3가지로 구분됩니다. 
1. Backbone
- Backbone은 input image를 feature map으로 변형하는 부분입니다.

2. Neck
- Neck은 Backbone과 Head를 연결하는 부분입니다. Feature map을 정제(Refinement), 재구성(Reconfiguration)한다고 생각하면 될 것 같습니다.

3. Head(Dense / Sparse Prediction)
- Head는 Backbone에서 추출한 feature map의 location 작업이 이뤄지는 곳으로 Predict classes와 bounding boxes 작업이 이뤄집니다. Head는 크게 두가지로 구분되는데 Dense Prediction과 Sparse Prediction입니다.  
Dense Prediction에서는 Predict와 Bounding boxes가 함께 작업이 되고, Sparse Prediction에서는 Predict와 Bounding Boxes가 구분되어 작업됩니다. 이때 Dense Prediction이 Head일 경우 Architecture 전체를 One-Stage Detector, Sparse Prediction이 Head일 경우 Two-Stage Detector가 됩니다.

#### 2.2 Bag of Freebies & 2.3 Bag of Specials
2.2절에서 2.3절까지는 BoF와 BoS의 기초적인 설명이라고 보면 될 것 같다.  
- BoF는 inference cost의 변화 없이 성능 향상을 꾀할 수 있는 딥러닝 기법이다. 대표적으로 데이터 증강(CutMix, Mosaic 등)과 BBox(Bounding Box) Regression의 손실 함수(IOU loss, CIOU loss 등)이 있습니다.  
- BOS는 BOF의 반대로 inference cost가 조금 상승하지만 성능 향상이 되는 딥러닝 기법이다. 대표적으로 enhance receptive filed(SPP, ASPP,RFB), feature integration(skip-connection, hyper-column, Bi-FPN) 그리고 최적의 activation function(P-ReLU, ReLU, Mish 등)이 있습니다.  
- - -
## 3. Methodology
#### 3.1 Selection of architecture & 3.2 Selection of BoF and BoS & 3.3 Additional improvements
3.1절에서 3.3절까지는 YOLOv4 모델 구성 선택 과정에 대한 설명이다. 3.4의 YOLOv4의 구성 요소만을 참고하면 될 것 같습니다.

#### 3.4 YOLOv4
YOLOv4에서는 Backbone에 CSPDarknet53, Neck에 SPP-net과 PANet, Head부분에는 YOLOv3을 사용했습니다.  

1. Backbone
Backbone으로는 CSPDarknet53이 사용되었다. CSPDarknet은 CSPDensenet의 Densenet자리에 Darknet구조를 사용한 것이다. 그럼 기존 Densenet과 CSPDensenet의 차이는 무엇일까?  

![SmartSelect_20201226-233836_Noteshelf](https://user-images.githubusercontent.com/39725476/103153460-f0e2d080-47d3-11eb-8e3a-08bb49bd9fc9.jpg)
![SmartSelect_20201226-233855_Noteshelf](https://user-images.githubusercontent.com/39725476/103153468-07892780-47d4-11eb-9b41-32137eadcb4a.jpg)


- - -

작성중
